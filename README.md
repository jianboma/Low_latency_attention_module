# Low latency attention module

This repo contains supplementary matrials for [Low latency transformers for speech processing](https://arxiv.org/abs/2302.13451).

The detailed development of the low latency attention module can be found in [low_latency_attention_module](doc/low_latency_attention_module.pdf).

## Citation

```BibTeX
@article{ma2023low,
  title={Low latency transformers for speech processing},
  author={Ma, Jianbo and Pan, Siqi and Chandran, Deepak and Fanelli, Andrea and Cartwright, Richard},
  journal={arXiv preprint arXiv:2302.13451},
  year={2023}
}
```